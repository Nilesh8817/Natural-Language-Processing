{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Text Preprocessing assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoX3S7o18nGr"
      },
      "source": [
        "<p><b>01. \n",
        "Perform tokenization, stemming and lemmatization, stopwords and punctuation removal on the given text ?</b></p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz1ZLbxUfe5j"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ikln1H9otgK",
        "outputId": "ece70611-ed30-4f57-9fe4-e5cabd57d2ec"
      },
      "source": [
        "nltk.download('wordnet') # Lemmatization\n",
        "nltk.download('punkt') # Tokenization \n",
        "nltk.download('stopwords') # stopwords"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCLXeqH6fe6k"
      },
      "source": [
        "doc = '''A major drawback of statistical methods is that they require elaborate feature engineering. Since the early 2010s,[16] \n",
        "the field has thus largely abandoned statistical methods and shifted to neural networks for machine learning. Popular techniques include \n",
        "the use of word embeddings to capture semantic properties of words, and an increase in end-to-end learning of a higher-level task \n",
        "(e.g., question answering) instead of relying on a pipeline of separate intermediate tasks (e.g., part-of-speech tagging and dependency parsing). \n",
        "In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches \n",
        "may be viewed as a new paradigm distinct from statistical natural language processing. For instance, the term neural machine translation (NMT) \n",
        "emphasizes the fact that deep learning-based approaches to machine translation directly learn sequence-to-sequence transformations, obviating \n",
        "the need for intermediate steps such as word alignment and language modeling that was used in statistical machine translation (SMT).'''"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1eKLbWzfe60"
      },
      "source": [
        "## Tokenization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBAOSseufe67"
      },
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgIvGshFfe68"
      },
      "source": [
        "tokens = word_tokenize(doc)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UC1_Sq2fe7A",
        "outputId": "6e637ea7-fd9c-4d63-f970-d6f6d38ed73c"
      },
      "source": [
        "tokens"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A',\n",
              " 'major',\n",
              " 'drawback',\n",
              " 'of',\n",
              " 'statistical',\n",
              " 'methods',\n",
              " 'is',\n",
              " 'that',\n",
              " 'they',\n",
              " 'require',\n",
              " 'elaborate',\n",
              " 'feature',\n",
              " 'engineering',\n",
              " '.',\n",
              " 'Since',\n",
              " 'the',\n",
              " 'early',\n",
              " '2010s',\n",
              " ',',\n",
              " '[',\n",
              " '16',\n",
              " ']',\n",
              " 'the',\n",
              " 'field',\n",
              " 'has',\n",
              " 'thus',\n",
              " 'largely',\n",
              " 'abandoned',\n",
              " 'statistical',\n",
              " 'methods',\n",
              " 'and',\n",
              " 'shifted',\n",
              " 'to',\n",
              " 'neural',\n",
              " 'networks',\n",
              " 'for',\n",
              " 'machine',\n",
              " 'learning',\n",
              " '.',\n",
              " 'Popular',\n",
              " 'techniques',\n",
              " 'include',\n",
              " 'the',\n",
              " 'use',\n",
              " 'of',\n",
              " 'word',\n",
              " 'embeddings',\n",
              " 'to',\n",
              " 'capture',\n",
              " 'semantic',\n",
              " 'properties',\n",
              " 'of',\n",
              " 'words',\n",
              " ',',\n",
              " 'and',\n",
              " 'an',\n",
              " 'increase',\n",
              " 'in',\n",
              " 'end-to-end',\n",
              " 'learning',\n",
              " 'of',\n",
              " 'a',\n",
              " 'higher-level',\n",
              " 'task',\n",
              " '(',\n",
              " 'e.g.',\n",
              " ',',\n",
              " 'question',\n",
              " 'answering',\n",
              " ')',\n",
              " 'instead',\n",
              " 'of',\n",
              " 'relying',\n",
              " 'on',\n",
              " 'a',\n",
              " 'pipeline',\n",
              " 'of',\n",
              " 'separate',\n",
              " 'intermediate',\n",
              " 'tasks',\n",
              " '(',\n",
              " 'e.g.',\n",
              " ',',\n",
              " 'part-of-speech',\n",
              " 'tagging',\n",
              " 'and',\n",
              " 'dependency',\n",
              " 'parsing',\n",
              " ')',\n",
              " '.',\n",
              " 'In',\n",
              " 'some',\n",
              " 'areas',\n",
              " ',',\n",
              " 'this',\n",
              " 'shift',\n",
              " 'has',\n",
              " 'entailed',\n",
              " 'substantial',\n",
              " 'changes',\n",
              " 'in',\n",
              " 'how',\n",
              " 'NLP',\n",
              " 'systems',\n",
              " 'are',\n",
              " 'designed',\n",
              " ',',\n",
              " 'such',\n",
              " 'that',\n",
              " 'deep',\n",
              " 'neural',\n",
              " 'network-based',\n",
              " 'approaches',\n",
              " 'may',\n",
              " 'be',\n",
              " 'viewed',\n",
              " 'as',\n",
              " 'a',\n",
              " 'new',\n",
              " 'paradigm',\n",
              " 'distinct',\n",
              " 'from',\n",
              " 'statistical',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " '.',\n",
              " 'For',\n",
              " 'instance',\n",
              " ',',\n",
              " 'the',\n",
              " 'term',\n",
              " 'neural',\n",
              " 'machine',\n",
              " 'translation',\n",
              " '(',\n",
              " 'NMT',\n",
              " ')',\n",
              " 'emphasizes',\n",
              " 'the',\n",
              " 'fact',\n",
              " 'that',\n",
              " 'deep',\n",
              " 'learning-based',\n",
              " 'approaches',\n",
              " 'to',\n",
              " 'machine',\n",
              " 'translation',\n",
              " 'directly',\n",
              " 'learn',\n",
              " 'sequence-to-sequence',\n",
              " 'transformations',\n",
              " ',',\n",
              " 'obviating',\n",
              " 'the',\n",
              " 'need',\n",
              " 'for',\n",
              " 'intermediate',\n",
              " 'steps',\n",
              " 'such',\n",
              " 'as',\n",
              " 'word',\n",
              " 'alignment',\n",
              " 'and',\n",
              " 'language',\n",
              " 'modeling',\n",
              " 'that',\n",
              " 'was',\n",
              " 'used',\n",
              " 'in',\n",
              " 'statistical',\n",
              " 'machine',\n",
              " 'translation',\n",
              " '(',\n",
              " 'SMT',\n",
              " ')',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zge8Sqxkfe7C",
        "outputId": "111d8d2e-ea4c-4cc2-f16e-4721a9df2927"
      },
      "source": [
        "len(tokens) # meaningful tokens, stop words, punctuations"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "177"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcNOw1lEDtix"
      },
      "source": [
        "## Stopwords removal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-K7qOBhufe7J"
      },
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcFFVCitfe7P"
      },
      "source": [
        "stop = stopwords.words('english')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNseX61wxHa0",
        "outputId": "352b9036-481d-45a3-9daf-54686d7500ba"
      },
      "source": [
        "stop"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdiCyKcy9sPC"
      },
      "source": [
        "## Punctuation removal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTX0LYfNy-fc"
      },
      "source": [
        "from string import punctuation"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LBbmM8e_zCmp",
        "outputId": "8f2bc99d-5df9-4b4d-8729-f9cb025856d3"
      },
      "source": [
        "punctuation"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX8vNKftfe7S"
      },
      "source": [
        "punc = list(punctuation)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3VGCJoDzPbg",
        "outputId": "d5a9040c-0083-4e5b-a515-183b2c5ff180"
      },
      "source": [
        "punc"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['!',\n",
              " '\"',\n",
              " '#',\n",
              " '$',\n",
              " '%',\n",
              " '&',\n",
              " \"'\",\n",
              " '(',\n",
              " ')',\n",
              " '*',\n",
              " '+',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '/',\n",
              " ':',\n",
              " ';',\n",
              " '<',\n",
              " '=',\n",
              " '>',\n",
              " '?',\n",
              " '@',\n",
              " '[',\n",
              " '\\\\',\n",
              " ']',\n",
              " '^',\n",
              " '_',\n",
              " '`',\n",
              " '{',\n",
              " '|',\n",
              " '}',\n",
              " '~']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D56nefLpfe7U"
      },
      "source": [
        "bad_tokens = stop + punc"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyVKLXC8fe7V",
        "outputId": "2a148b0c-2107-4955-c2e3-2694f71b0f46"
      },
      "source": [
        "bad_tokens"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " '!',\n",
              " '\"',\n",
              " '#',\n",
              " '$',\n",
              " '%',\n",
              " '&',\n",
              " \"'\",\n",
              " '(',\n",
              " ')',\n",
              " '*',\n",
              " '+',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '/',\n",
              " ':',\n",
              " ';',\n",
              " '<',\n",
              " '=',\n",
              " '>',\n",
              " '?',\n",
              " '@',\n",
              " '[',\n",
              " '\\\\',\n",
              " ']',\n",
              " '^',\n",
              " '_',\n",
              " '`',\n",
              " '{',\n",
              " '|',\n",
              " '}',\n",
              " '~']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2h2VPT5ofe7Y"
      },
      "source": [
        "clean_tokens = [t for t in tokens if t not in bad_tokens]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhO5Bo6Ife7a",
        "outputId": "71e3aa86-1569-45d3-856f-1a3e4394d164"
      },
      "source": [
        "clean_tokens"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A',\n",
              " 'major',\n",
              " 'drawback',\n",
              " 'statistical',\n",
              " 'methods',\n",
              " 'require',\n",
              " 'elaborate',\n",
              " 'feature',\n",
              " 'engineering',\n",
              " 'Since',\n",
              " 'early',\n",
              " '2010s',\n",
              " '16',\n",
              " 'field',\n",
              " 'thus',\n",
              " 'largely',\n",
              " 'abandoned',\n",
              " 'statistical',\n",
              " 'methods',\n",
              " 'shifted',\n",
              " 'neural',\n",
              " 'networks',\n",
              " 'machine',\n",
              " 'learning',\n",
              " 'Popular',\n",
              " 'techniques',\n",
              " 'include',\n",
              " 'use',\n",
              " 'word',\n",
              " 'embeddings',\n",
              " 'capture',\n",
              " 'semantic',\n",
              " 'properties',\n",
              " 'words',\n",
              " 'increase',\n",
              " 'end-to-end',\n",
              " 'learning',\n",
              " 'higher-level',\n",
              " 'task',\n",
              " 'e.g.',\n",
              " 'question',\n",
              " 'answering',\n",
              " 'instead',\n",
              " 'relying',\n",
              " 'pipeline',\n",
              " 'separate',\n",
              " 'intermediate',\n",
              " 'tasks',\n",
              " 'e.g.',\n",
              " 'part-of-speech',\n",
              " 'tagging',\n",
              " 'dependency',\n",
              " 'parsing',\n",
              " 'In',\n",
              " 'areas',\n",
              " 'shift',\n",
              " 'entailed',\n",
              " 'substantial',\n",
              " 'changes',\n",
              " 'NLP',\n",
              " 'systems',\n",
              " 'designed',\n",
              " 'deep',\n",
              " 'neural',\n",
              " 'network-based',\n",
              " 'approaches',\n",
              " 'may',\n",
              " 'viewed',\n",
              " 'new',\n",
              " 'paradigm',\n",
              " 'distinct',\n",
              " 'statistical',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'For',\n",
              " 'instance',\n",
              " 'term',\n",
              " 'neural',\n",
              " 'machine',\n",
              " 'translation',\n",
              " 'NMT',\n",
              " 'emphasizes',\n",
              " 'fact',\n",
              " 'deep',\n",
              " 'learning-based',\n",
              " 'approaches',\n",
              " 'machine',\n",
              " 'translation',\n",
              " 'directly',\n",
              " 'learn',\n",
              " 'sequence-to-sequence',\n",
              " 'transformations',\n",
              " 'obviating',\n",
              " 'need',\n",
              " 'intermediate',\n",
              " 'steps',\n",
              " 'word',\n",
              " 'alignment',\n",
              " 'language',\n",
              " 'modeling',\n",
              " 'used',\n",
              " 'statistical',\n",
              " 'machine',\n",
              " 'translation',\n",
              " 'SMT']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gAn6y2kfe7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d7e5eef-807d-44bb-de80-c46f0882d126"
      },
      "source": [
        "len(clean_tokens)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "106"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWIjwMKXfe7c"
      },
      "source": [
        "## Stemming."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4C5oS0SJfe7i"
      },
      "source": [
        "stemmer = nltk.PorterStemmer()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eutscODo6GX-"
      },
      "source": [
        "stemmer_tokens = [[stemmer.stem(w) for w in clean_tokens]]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nS6pE9UZfe7l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d030b82e-89b8-4fc0-f076-fb4a91820743"
      },
      "source": [
        "stemmer_tokens"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['A',\n",
              "  'major',\n",
              "  'drawback',\n",
              "  'statist',\n",
              "  'method',\n",
              "  'requir',\n",
              "  'elabor',\n",
              "  'featur',\n",
              "  'engin',\n",
              "  'sinc',\n",
              "  'earli',\n",
              "  '2010',\n",
              "  '16',\n",
              "  'field',\n",
              "  'thu',\n",
              "  'larg',\n",
              "  'abandon',\n",
              "  'statist',\n",
              "  'method',\n",
              "  'shift',\n",
              "  'neural',\n",
              "  'network',\n",
              "  'machin',\n",
              "  'learn',\n",
              "  'popular',\n",
              "  'techniqu',\n",
              "  'includ',\n",
              "  'use',\n",
              "  'word',\n",
              "  'embed',\n",
              "  'captur',\n",
              "  'semant',\n",
              "  'properti',\n",
              "  'word',\n",
              "  'increas',\n",
              "  'end-to-end',\n",
              "  'learn',\n",
              "  'higher-level',\n",
              "  'task',\n",
              "  'e.g.',\n",
              "  'question',\n",
              "  'answer',\n",
              "  'instead',\n",
              "  'reli',\n",
              "  'pipelin',\n",
              "  'separ',\n",
              "  'intermedi',\n",
              "  'task',\n",
              "  'e.g.',\n",
              "  'part-of-speech',\n",
              "  'tag',\n",
              "  'depend',\n",
              "  'pars',\n",
              "  'In',\n",
              "  'area',\n",
              "  'shift',\n",
              "  'entail',\n",
              "  'substanti',\n",
              "  'chang',\n",
              "  'nlp',\n",
              "  'system',\n",
              "  'design',\n",
              "  'deep',\n",
              "  'neural',\n",
              "  'network-bas',\n",
              "  'approach',\n",
              "  'may',\n",
              "  'view',\n",
              "  'new',\n",
              "  'paradigm',\n",
              "  'distinct',\n",
              "  'statist',\n",
              "  'natur',\n",
              "  'languag',\n",
              "  'process',\n",
              "  'for',\n",
              "  'instanc',\n",
              "  'term',\n",
              "  'neural',\n",
              "  'machin',\n",
              "  'translat',\n",
              "  'nmt',\n",
              "  'emphas',\n",
              "  'fact',\n",
              "  'deep',\n",
              "  'learning-bas',\n",
              "  'approach',\n",
              "  'machin',\n",
              "  'translat',\n",
              "  'directli',\n",
              "  'learn',\n",
              "  'sequence-to-sequ',\n",
              "  'transform',\n",
              "  'obviat',\n",
              "  'need',\n",
              "  'intermedi',\n",
              "  'step',\n",
              "  'word',\n",
              "  'align',\n",
              "  'languag',\n",
              "  'model',\n",
              "  'use',\n",
              "  'statist',\n",
              "  'machin',\n",
              "  'translat',\n",
              "  'smt']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3___MV6fe7q"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKdogbZ_fe7t"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Fx2vy0nfe7w"
      },
      "source": [
        "lemma = WordNetLemmatizer()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meVrvpiBfe75"
      },
      "source": [
        "lemmatized_tokens = [lemma.lemmatize(t) for t in clean_tokens]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyigP7F5fe8B",
        "outputId": "6de489a2-f8c9-4940-9968-91f288384aec"
      },
      "source": [
        "lemmatized_tokens"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A',\n",
              " 'major',\n",
              " 'drawback',\n",
              " 'statistical',\n",
              " 'method',\n",
              " 'require',\n",
              " 'elaborate',\n",
              " 'feature',\n",
              " 'engineering',\n",
              " 'Since',\n",
              " 'early',\n",
              " '2010s',\n",
              " '16',\n",
              " 'field',\n",
              " 'thus',\n",
              " 'largely',\n",
              " 'abandoned',\n",
              " 'statistical',\n",
              " 'method',\n",
              " 'shifted',\n",
              " 'neural',\n",
              " 'network',\n",
              " 'machine',\n",
              " 'learning',\n",
              " 'Popular',\n",
              " 'technique',\n",
              " 'include',\n",
              " 'use',\n",
              " 'word',\n",
              " 'embeddings',\n",
              " 'capture',\n",
              " 'semantic',\n",
              " 'property',\n",
              " 'word',\n",
              " 'increase',\n",
              " 'end-to-end',\n",
              " 'learning',\n",
              " 'higher-level',\n",
              " 'task',\n",
              " 'e.g.',\n",
              " 'question',\n",
              " 'answering',\n",
              " 'instead',\n",
              " 'relying',\n",
              " 'pipeline',\n",
              " 'separate',\n",
              " 'intermediate',\n",
              " 'task',\n",
              " 'e.g.',\n",
              " 'part-of-speech',\n",
              " 'tagging',\n",
              " 'dependency',\n",
              " 'parsing',\n",
              " 'In',\n",
              " 'area',\n",
              " 'shift',\n",
              " 'entailed',\n",
              " 'substantial',\n",
              " 'change',\n",
              " 'NLP',\n",
              " 'system',\n",
              " 'designed',\n",
              " 'deep',\n",
              " 'neural',\n",
              " 'network-based',\n",
              " 'approach',\n",
              " 'may',\n",
              " 'viewed',\n",
              " 'new',\n",
              " 'paradigm',\n",
              " 'distinct',\n",
              " 'statistical',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'For',\n",
              " 'instance',\n",
              " 'term',\n",
              " 'neural',\n",
              " 'machine',\n",
              " 'translation',\n",
              " 'NMT',\n",
              " 'emphasizes',\n",
              " 'fact',\n",
              " 'deep',\n",
              " 'learning-based',\n",
              " 'approach',\n",
              " 'machine',\n",
              " 'translation',\n",
              " 'directly',\n",
              " 'learn',\n",
              " 'sequence-to-sequence',\n",
              " 'transformation',\n",
              " 'obviating',\n",
              " 'need',\n",
              " 'intermediate',\n",
              " 'step',\n",
              " 'word',\n",
              " 'alignment',\n",
              " 'language',\n",
              " 'modeling',\n",
              " 'used',\n",
              " 'statistical',\n",
              " 'machine',\n",
              " 'translation',\n",
              " 'SMT']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ud4R-wA43N6"
      },
      "source": [
        ""
      ],
      "execution_count": 26,
      "outputs": []
    }
  ]
}